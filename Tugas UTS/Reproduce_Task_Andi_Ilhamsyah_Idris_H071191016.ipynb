{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reproduce Task - Andi Ilhamsyah Idris H071191016.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "D4DcVGyYDkwh"
      },
      "outputs": [],
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "import numpy as np \n",
        "import random\n",
        "from PIL import Image\n",
        "from types import SimpleNamespace\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.reset_orig()\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "# Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path folder dimana datasets didownload\n",
        "DATASET_PATH = \"../data\"\n",
        "# Path folder dimana models akan disimpan\n",
        "CHECKPOINT_PATH = \"../saved_models/\"\n",
        "\n",
        "# Fungsi untuk mengatur seed\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "set_seed(42)\n",
        "\n",
        "# Memastikan semua operasi deterministik dengan GPU untuk reproduktivitas\n",
        "torch.backends.cudnn.determinstic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "BLsSQlf1RGpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "from urllib.error import HTTPError\n",
        "# Github URL dimana models disimpan\n",
        "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial5/\"\n",
        "# File yang didownload\n",
        "pretrained_files = [\"GoogleNet.ckpt\", \"ResNet.ckpt\", \"ResNetPreAct.ckpt\", \"DenseNet.ckpt\",\n",
        "                    \"tensorboards/GoogleNet/events.out.tfevents.googlenet\",\n",
        "                    \"tensorboards/ResNet/events.out.tfevents.resnet\",\n",
        "                    \"tensorboards/ResNetPreAct/events.out.tfevents.resnetpreact\",\n",
        "                    \"tensorboards/DenseNet/events.out.tfevents.densenet\"]\n",
        "# Buat tanda path jika tidak dapat diakses\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "\n",
        "# Setiap file diperiksa jika sudah ada. Jika tidak, file akan diunduh.\n",
        "for file_name in pretrained_files:\n",
        "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
        "    if \"/\" in file_name:\n",
        "        os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n",
        "    if not os.path.isfile(file_path):\n",
        "        file_url = base_url + file_name\n",
        "        print(f\"Downloading {file_url}...\")\n",
        "        try:\n",
        "            urllib.request.urlretrieve(file_url, file_path)\n",
        "        except HTTPError as e:\n",
        "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including th"
      ],
      "metadata": {
        "id": "Agwep_a7S64D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CIFAR10(root=DATASET_PATH, train=True, download=True)"
      ],
      "metadata": {
        "id": "z_tUf0PmT_H5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_MEANS = (train_dataset.data / 255.0).mean(axis=(0,1,2))\n",
        "DATA_STD = (train_dataset.data / 255.0).std(axis=(0,1,2))\n",
        "\n",
        "print(\"Data mean\", DATA_MEANS)\n",
        "print(\"Data std\", DATA_STD)"
      ],
      "metadata": {
        "id": "qXOSBkwIVXh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize(DATA_MEANS, DATA_STD)\n",
        "                                     ])\n",
        "# Untuk latihan, kita menambahkan sebuah augmentasi, Jaringan terlalu kuat dan akan overfit.\n",
        "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.RandomResizedCrop((32,32),scale=(0.8,1.0),ratio=(0.9,1.1)),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(DATA_MEANS, DATA_STD)\n",
        "                                     ])\n",
        "# Memuat set data pelatihan. Kita perlu membaginya menjadi bagian pelatihan dan validasi\n",
        "# Kita perlu melakukan sedikit trik karena set validasi tidak boleh menggunakan augmentasi.\n",
        "train_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=train_transform, download=True)\n",
        "val_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=test_transform, download=True)\n",
        "set_seed(42)\n",
        "train_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
        "set_seed(42)\n",
        "_, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000])\n",
        "\n",
        "# Memuat set test\n",
        "test_set = CIFAR10(root=DATASET_PATH, train=False, transform=test_transform, download=True)\n",
        "\n",
        "# Kami mendefinisikan satu set pemuat data yang dapat kami gunakan untuk berbagai keperluan nanti.\n",
        "train_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
        "val_loader = data.DataLoader(val_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)\n",
        "test_loader = data.DataLoader(test_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)"
      ],
      "metadata": {
        "id": "k4nDmfBiZMPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgs, _ = next(iter(train_loader))\n",
        "print(\"Batch mean\", imgs.mean(dim=[0,2,3]))\n",
        "print(\"Batch std\", imgs.std(dim=[0,2,3]))"
      ],
      "metadata": {
        "id": "IcW1Q9uzaBBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_IMAGES = 4\n",
        "images = [train_dataset[idx][0] for idx in range(NUM_IMAGES)]\n",
        "orig_images = [Image.fromarray(train_dataset.data[idx]) for idx in range(NUM_IMAGES)]\n",
        "orig_images = [test_transform(img) for img in orig_images]\n",
        "\n",
        "img_grid = torchvision.utils.make_grid(torch.stack(images + orig_images, dim=0), nrow=4, normalize=True, pad_value=0.5)\n",
        "img_grid = img_grid.permute(1, 2, 0)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.title(\"Augmentation examples on CIFAR10\")\n",
        "plt.imshow(img_grid)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "ZGv0llvmcCa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab tidak menyediakan PyTorch Lightning instalan default. Maka dari itu, kami melakukannya di sini jika perlu\n",
        "    !pip install --quiet pytorch-lightning>=1.4\n",
        "    import pytorch_lightning as pl"
      ],
      "metadata": {
        "id": "Cx_AayEMdY1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengatur seed\n",
        "pl.seed_everything(42)"
      ],
      "metadata": {
        "id": "ePDdmGQle0fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFARModule(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, model_name, model_hparams, optimizer_name, optimizer_hparams):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            model_name - Name of the model/CNN to run. Used for creating the model (see function below)\n",
        "            model_hparams - Hyperparameters for the model, as dictionary.\n",
        "            optimizer_name - Name of the optimizer to use. Currently supported: Adam, SGD\n",
        "            optimizer_hparams - Hyperparameters for the optimizer, as dictionary. This includes learning rate, weight decay, etc.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Export hyperparamater ke file YAML dan buat \"self.hparams\" sebagai nama\n",
        "        self.save_hyperparameters()\n",
        "        # Buat model\n",
        "        self.model = create_model(model_name, model_hparams)\n",
        "        # Buat loss module\n",
        "        self.loss_module = nn.CrossEntropyLoss()\n",
        "        # Example input untuk visualisasi grafik di Tensorboard\n",
        "        self.example_input_array = torch.zeros((1, 3, 32, 32), dtype=torch.float32)\n",
        "\n",
        "    def forward(self, imgs):\n",
        "        # Forward function berjalan ketika mengvisualisasikan grafik\n",
        "        return self.model(imgs)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # Kita akan support Adam atau SGD sebagai optimalisasi.\n",
        "        if self.hparams.optimizer_name == \"Adam\":\n",
        "            # AdamW adalah Adam dengan implementasi yang benar terhadap penurunan beban (see here for details: https://arxiv.org/pdf/1711.05101.pdf)\n",
        "            optimizer = optim.AdamW(\n",
        "                self.parameters(), **self.hparams.optimizer_hparams)\n",
        "        elif self.hparams.optimizer_name == \"SGD\":\n",
        "            optimizer = optim.SGD(self.parameters(), **self.hparams.optimizer_hparams)\n",
        "        else:\n",
        "            assert False, f\"Unknown optimizer: \\\"{self.hparams.optimizer_name}\\\"\"\n",
        "\n",
        "        # Kami akan menurunkan learning rate sebanyak 0.1 setelah 100 dan 150 epochs\n",
        "        scheduler = optim.lr_scheduler.MultiStepLR(\n",
        "            optimizer, milestones=[100, 150], gamma=0.1)\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # \"batch\" adalah output dari training data loader.\n",
        "        imgs, labels = batch\n",
        "        preds = self.model(imgs)\n",
        "        loss = self.loss_module(preds, labels)\n",
        "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
        "\n",
        "        # Logs the accuracy per epoch to tensorboard (weighted average over batches)\n",
        "        self.log('train_acc', acc, on_step=False, on_epoch=True)\n",
        "        self.log('train_loss', loss)\n",
        "        return loss  # Return tensor to call \".backward\" on\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        imgs, labels = batch\n",
        "        preds = self.model(imgs).argmax(dim=-1)\n",
        "        acc = (labels == preds).float().mean()\n",
        "        # By default logs it per epoch (weighted average over batches)\n",
        "        self.log('val_acc', acc)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        imgs, labels = batch\n",
        "        preds = self.model(imgs).argmax(dim=-1)\n",
        "        acc = (labels == preds).float().mean()\n",
        "        # By default logs it per epoch (weighted average over batches), and returns it afterwards\n",
        "        self.log('test_acc', acc)"
      ],
      "metadata": {
        "id": "RPlNHHurgUO7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}